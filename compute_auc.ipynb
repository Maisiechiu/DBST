{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSH 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取第一个检测器的预测结果文件\n",
    "detector1_file_path = '/home/jovyan/temporaldeepfake/mmaction2/auc_results_fsh_onlytrain_full.txt'\n",
    "video_frame_scores = defaultdict(list)\n",
    "\n",
    "with open(detector1_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 3:\n",
    "            full_path = parts[0]\n",
    "            score = float(parts[1])\n",
    "            ground_truth = int(parts[2])\n",
    "\n",
    "            # 提取影片、frame和face id\n",
    "            path_parts = full_path.split('/')\n",
    "            video_id = path_parts[-3]  # 影片ID，例如000_003\n",
    "            frame_id = path_parts[-2]  # frame ID，例如008\n",
    "\n",
    "            # 使用影片和frame ID作为键来存储预测分数\n",
    "            video_frame_scores[(video_id, frame_id)].append(score)\n",
    "\n",
    "# 计算每个影片的frame的最大预测分数的平均值\n",
    "video_scores_detector1 = defaultdict(list)\n",
    "for (video_id, frame_id), scores in video_frame_scores.items():\n",
    "    max_score = max(scores)\n",
    "    video_scores_detector1[video_id].append(max_score)\n",
    "\n",
    "# 计算每个影片的最终预测结果\n",
    "final_video_scores_detector1 = {video_id: sum(scores) / len(scores) for video_id, scores in video_scores_detector1.items()}\n",
    "\n",
    "# # 读取第二个检测器的预测结果文件\n",
    "detector2_file_path = '/home/jovyan/temporaldeepfake/mmaction2/SBI_supcon_contrast0.5_2opt0.01_without45aug_base_07_17_12_58_19_weights_46_0.7688_val.tar_FF-FH_predict.txt'\n",
    "video_scores_detector2 = {}\n",
    "\n",
    "with open(detector2_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            video_id = parts[0]\n",
    "            score = float(parts[1])\n",
    "            video_scores_detector2[video_id] = score\n",
    "\n",
    "# Initialize combined_video_scores as a defaultdict of lists\n",
    "combined_video_scores = defaultdict(list)\n",
    "\n",
    "# Add scores from the first detector\n",
    "for video_id, score in final_video_scores_detector1.items():\n",
    "    combined_video_scores[video_id].append(score)\n",
    "\n",
    "# Add scores from the second detector\n",
    "for video_id, score in video_scores_detector2.items():\n",
    "    combined_video_scores[video_id].append(score)\n",
    "\n",
    "# Calculate the final score for each video by taking the mean of all available scores\n",
    "final_combined_scores = {video_id: np.mean(scores) for video_id, scores in combined_video_scores.items()}\n",
    "\n",
    "# Preparing labels and predictions for AUC calculation\n",
    "true_labels = []\n",
    "pred_scores = []\n",
    "\n",
    "for video_id, score in final_combined_scores.items():\n",
    "    label = 1 if '_' in video_id else 0\n",
    "    true_labels.append(label)\n",
    "    pred_scores.append(score)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celebdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取第一个检测器的预测结果文件\n",
    "\n",
    "best_auc = 0\n",
    "best_k = 0\n",
    "\n",
    "for k in np.arange(0, 1.1, 0.1):\n",
    "    detector1_file_path = \"/home/jovyan/temporaldeepfake/mmaction2/20240925_071635_CDF_random_margin_5frames.txt\"\n",
    "    video_frame_scores = defaultdict(list)\n",
    "    with open(detector1_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                full_path = parts[0]\n",
    "                score = float(parts[1])\n",
    "                ground_truth = int(parts[2])\n",
    "\n",
    "                # 提取影片、frame和face id\n",
    "                path_parts = full_path.split('/')\n",
    "                video_id = path_parts[-3]  # 影片ID，例如000_003\n",
    "                frame_id = path_parts[-2]  # frame ID，例如008\n",
    "\n",
    "                # 使用影片和frame ID作为键来存储预测分数\n",
    "                video_frame_scores[(video_id, frame_id)].append(score)\n",
    "\n",
    "    # 计算每个影片的frame的最大预测分数的平均值\n",
    "    video_scores_detector1 = defaultdict(list)\n",
    "    for (video_id, frame_id), scores in video_frame_scores.items():\n",
    "        max_score = scores[0]\n",
    "        video_scores_detector1[video_id].append(max_score)\n",
    "\n",
    "    # 计算每个影片的最终预测结果\n",
    "    # final_video_scores_detector1 = {}\n",
    "    # for video_id, scores in video_scores_detector1.items():\n",
    "    #     top_k_scores = sorted(scores, reverse=True)[:k]\n",
    "    #     average_top_k = sum(top_k_scores) / len(top_k_scores)\n",
    "    #     final_video_scores_detector1[video_id] = average_top_k\n",
    "    final_video_scores_detector1 = {video_id: sum(scores) / len(scores) for video_id, scores in video_scores_detector1.items()}\n",
    "\n",
    "\n",
    "    # # 读取第二个检测器的预测结果文件\n",
    "    detector2_file_path = '/home/jovyan/temporaldeepfake/auc_results/spatial/SBI_base_09_02_13_37_12_weights_63_0.7780_val.tar_Celeb-DF-v2_predict_top128_predictions.txt'\n",
    "    video_scores_detector2 = {}\n",
    "\n",
    "    with open(detector2_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            video_id = parts[0]\n",
    "            score = float(parts[1])\n",
    "            video_scores_detector2[video_id] = score\n",
    "\n",
    "    # Initialize combined_video_scores as a defaultdict of lists\n",
    "    combined_video_scores = defaultdict(list)\n",
    "\n",
    "    #Add scores from the first detector\n",
    "    for video_id, score in final_video_scores_detector1.items():\n",
    "        combined_video_scores[video_id].append(k*score)\n",
    "\n",
    "    # # Add scores from the second detector\n",
    "    for video_id, score in video_scores_detector2.items():\n",
    "        combined_video_scores[video_id].append((1-k)*score)\n",
    "\n",
    "    # Calculate the final score for each video by taking the mean of all available scores\n",
    "    final_combined_scores = {video_id: np.sum(scores) for video_id, scores in combined_video_scores.items()}\n",
    "\n",
    "    # Preparing labels and predictions for AUC calculation\n",
    "    true_labels = []\n",
    "    pred_scores = []\n",
    "\n",
    "    for video_id, score in final_combined_scores.items():\n",
    "        label = 1 if video_id.count(\"id\") == 2 else 0  # 假设video_id中含有两个下划线表示id\n",
    "        true_labels.append(label)\n",
    "        pred_scores.append(score)\n",
    "\n",
    "    # Calculate AUC\n",
    "    if len(set(true_labels)) > 1:  # 确保标签中有至少两个不同值\n",
    "        auc = roc_auc_score(true_labels, pred_scores)\n",
    "        print(auc, len(true_labels))\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_k = k\n",
    "    else:\n",
    "        print(f\"top{k}: Not enough different labels to calculate AUC\")\n",
    "print(f\"Highest AUC: {best_auc} at temporal ratio: {best_k}, spatial ratio: {1-best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 假设文件路径\n",
    "file_path = \"/home/jovyan/temporaldeepfake/mmaction2/20240903_141415_CDF_random_margin_5frames.txt\"\n",
    "\n",
    "# 初始化字典\n",
    "grouped_scores = defaultdict(list)\n",
    "labels = {}\n",
    "\n",
    "# 读取和处理文件\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "for entry in data:\n",
    "    parts = entry.split()\n",
    "    path = parts[0]\n",
    "    score = float(parts[1])\n",
    "    label = int(parts[2])\n",
    "\n",
    "    # 获取父文件夹路径\n",
    "    parent_folder = path.split('/')[-3]\n",
    "    \n",
    "    # 收集相同父文件夹的分数\n",
    "    grouped_scores[parent_folder].append(score)\n",
    "    \n",
    "    # 保存父文件夹对应的标签\n",
    "    if parent_folder not in labels:\n",
    "        labels[parent_folder] = label\n",
    "\n",
    "average_scores = []\n",
    "average_labels = []\n",
    "\n",
    "# 计算每个父文件夹的平均分数\n",
    "for parent_folder, scores in grouped_scores.items():\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    average_scores.append(average_score)\n",
    "    average_labels.append(labels[parent_folder])\n",
    "\n",
    "\n",
    "# 计算AUC\n",
    "if len(set(average_labels)) > 1:  # 确保标签有多个值\n",
    "    auc = roc_auc_score(average_labels, average_scores)\n",
    "    print(f\"AUC: {auc}\")\n",
    "else:\n",
    "    print(\"AUC cannot be computed (insufficient label variance)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import ast\n",
    "\n",
    "# 讀取txt內容\n",
    "video_predictions = defaultdict(list)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "# 读取文件内容\n",
    "file_path = '/home/jovyan/temporaldeepfake/auc_results/spatial/SBI_base_09_02_13_37_12_weights_63_0.7780_val.tar_Celeb-DF-v2_predict.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 處理每一行\n",
    "for line in lines:\n",
    "    parts = line.split()\n",
    "    video_id = parts[0]\n",
    "    probabilities = parts[2]\n",
    "    video_predictions[video_id].append(float(probabilities))\n",
    "    if video_id not in labels_dict:\n",
    "        labels_dict[video_id] = 1 if video_id.count('id')==2 else 0\n",
    "print(len(video_predictions))\n",
    "\n",
    "# 計算每個video從1到32的預測topk的平均並寫入文件\n",
    "for k in [1 , 32 , 64 , 128]:\n",
    "    topk_averages = {}\n",
    "    for video_id, predictions in video_predictions.items():\n",
    "        sorted_predictions = sorted(predictions, reverse=True)\n",
    "        topk_averages[video_id] = np.mean(sorted_predictions[:k])\n",
    "    \n",
    "    labels = []\n",
    "    scores = []\n",
    "    with open(f'SBI_base_09_02_13_37_12_weights_63_0.7780_val.tar_Celeb-DF-v2_predict_top{k}_predictions.txt', 'w') as output_file:\n",
    "        for video_id, avg_score in topk_averages.items():\n",
    "            label = labels_dict[video_id]\n",
    "            labels.append(label)\n",
    "            scores.append(avg_score)\n",
    "            output_file.write(f'{video_id} {avg_score} {label}\\n')\n",
    "    \n",
    "    # 計算並打印AUC\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    print(f\"K={k}, AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sptial topk + temporal topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import ast\n",
    "\n",
    "############################# spatial ################################\n",
    "video_predictions = defaultdict(list)\n",
    "spatial_predictions = defaultdict(int)\n",
    "labels = {}\n",
    "\n",
    "# 读取文件内容\n",
    "file_path = '/home/jovyan/temporaldeepfake/auc_results/spatial/only_train_without45_base_08_05_01_40_56_weights_22_0.7602_val.tar_Celeb-DF-v2_predict.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 處理每一行\n",
    "for line in lines:\n",
    "    parts = line.split(maxsplit=2)  # 只分割前两部分，保留预测值的完整性\n",
    "    video_id = parts[0]\n",
    "    probabilities = ast.literal_eval(parts[2])\n",
    "    if isinstance(probabilities, list):\n",
    "        max_prediction = max(probabilities)\n",
    "    else:\n",
    "        max_prediction = probabilities\n",
    "    video_predictions[video_id].append(max_prediction)\n",
    "        # 生成标签：包含两个不同ID的标为1，否则为0\n",
    "    if video_id not in labels:\n",
    "        labels[video_id] = 1 if video_id.count('id')==2 else 0\n",
    "\n",
    "# 計算每個video從1到32的預測topk的平均\n",
    "for k in range(1, 130):\n",
    "    topk_scores = []\n",
    "    topk_labels = []\n",
    "    \n",
    "for video_id, predictions in video_predictions.items():\n",
    "    # 对每个视频的预测分数进行排序\n",
    "    sorted_predictions = sorted(predictions, reverse=True)\n",
    "    # 取前k个分数的平均值\n",
    "    topk_avg = sum(sorted_predictions[:k]) / k\n",
    "    topk_scores.append(topk_avg)\n",
    "    topk_labels.append(labels[video_id])\n",
    "\n",
    "# 计算 AUC 分数\n",
    "if len(set(topk_labels)) > 1:  # 确保标签有多样性\n",
    "    auc_score = roc_auc_score(topk_labels, topk_scores)\n",
    "    print(f\"Top-{k} AUC: {auc_score:.4f}\")\n",
    "else:\n",
    "    print(f\"Top-{k} AUC cannot be computed due to insufficient label variance\")\n",
    "        \n",
    "###########################################temporal############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFDC 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def read_gt_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    real_list = df.loc[df['label'] == 0, 'filename'].tolist()\n",
    "    real_list = [video.split(\".\")[0] for video in real_list]\n",
    "    fake_list = df.loc[df['label'] == 1, 'filename'].tolist()\n",
    "    fake_list = [video.split(\".\")[0] for video in fake_list]\n",
    "    return real_list, fake_list\n",
    "\n",
    "real_list, fake_list = read_gt_csv(\"/home/jovyan/dataset/DFDC/test/labels.csv\")\n",
    "\n",
    "best_auc = 0\n",
    "best_k = 0\n",
    "\n",
    "detector1_file_path = '/home/jovyan/temporaldeepfake/mmaction2/20240925_075400_CDF_random_margin_5frames.txt'\n",
    "\n",
    "for k in np.arange(0, 1.1, 0.1):\n",
    "    video_frame_scores = defaultdict(list)  # 重置字典\n",
    "\n",
    "    with open(detector1_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                full_path = parts[0]\n",
    "                score = float(parts[1])\n",
    "                ground_truth = int(parts[2])\n",
    "\n",
    "                path_parts = full_path.split('/')\n",
    "                video_id = path_parts[-3]\n",
    "                frame_id = path_parts[-2]\n",
    "\n",
    "                video_frame_scores[(video_id, frame_id)].append(score)\n",
    "\n",
    "    video_scores_detector1 = defaultdict(list)\n",
    "    for (video_id, frame_id), scores in video_frame_scores.items():\n",
    "        max_score = scores[0]\n",
    "        video_scores_detector1[video_id].append(max_score)\n",
    "\n",
    "    final_video_scores_detector1 = {video_id: sum(scores) / len(scores) for video_id, scores in video_scores_detector1.items()}\n",
    "\n",
    "    detector2_file_path = '/home/jovyan/temporaldeepfake/auc_results/spatial/SBI_supcon_contrast0.5_2opt0.01_without45aug_base_07_17_12_58_19_weights_46_0.7688_val.tar_DFDC_predict.txt'\n",
    "    video_scores_detector2 = {}\n",
    "\n",
    "    with open(detector2_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                video_id = parts[0]\n",
    "                score = float(parts[1])\n",
    "                video_scores_detector2[video_id] = score\n",
    "\n",
    "    combined_video_scores = defaultdict(list)\n",
    "\n",
    "    for video_id, score in final_video_scores_detector1.items():\n",
    "        combined_video_scores[video_id].append(k * score)\n",
    "\n",
    "    # for video_id, score in video_scores_detector2.items():\n",
    "    #     combined_video_scores[video_id].append((1 - k) * score)\n",
    "\n",
    "    final_combined_scores = {video_id: np.mean(scores) for video_id, scores in combined_video_scores.items()}\n",
    "\n",
    "    true_labels = []\n",
    "    pred_scores = []\n",
    "\n",
    "    for video_id, score in final_combined_scores.items():\n",
    "        label = 0 if video_id in real_list else 1\n",
    "        true_labels.append(label)\n",
    "        pred_scores.append(score)\n",
    "\n",
    "    if len(set(true_labels)) > 1:\n",
    "        auc = roc_auc_score(true_labels, pred_scores)\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_k = k\n",
    "        print(k , auc)\n",
    "    else:\n",
    "        print(f\"top{k}: Not enough different labels to calculate AUC\")\n",
    "\n",
    "print(f\"Highest AUC: {best_auc} at temporal ratio: {best_k}, spatial ratio: {1-best_k}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFD with identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取第一个检测器的预测结果文件\n",
    "detector1_file_path = '/home/jovyan/temporaldeepfake/mmaction2/20240813_082409_CDF_wotest_woid.txt'\n",
    "video_frame_scores = defaultdict(list)\n",
    "video_frame_ground_truth = defaultdict(int)\n",
    "with open(detector1_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 3:\n",
    "            full_path = parts[0]\n",
    "            score = float(parts[1])\n",
    "            ground_truth = int(parts[2])\n",
    "\n",
    "            # 提取影片、frame和face id\n",
    "            path_parts = full_path.split('/')\n",
    "            video_id = path_parts[-3]  # 影片ID，例如000_003\n",
    "            frame_id = path_parts[-2]  # frame ID，例如008\n",
    "\n",
    "            # 使用影片和frame ID作为键来存储预测分数\n",
    "            video_frame_scores[(video_id, frame_id)].append(score)\n",
    "            video_frame_ground_truth[video_id] = ground_truth\n",
    "\n",
    "# 计算每个影片的frame的最大预测分数的平均值\n",
    "video_scores_detector1 = defaultdict(list)\n",
    "for (video_id, frame_id), scores in video_frame_scores.items():\n",
    "    max_score = max(scores)\n",
    "    video_scores_detector1[video_id].append(max_score)\n",
    "\n",
    "\n",
    "# 计算每个影片的最终预测结果\n",
    "final_video_scores_detector1 = {video_id: sum(scores) / len(scores) for video_id, scores in video_scores_detector1.items()}\n",
    "\n",
    "\n",
    "detector2_file_path = '/home/jovyan/temporaldeepfake/top60_predictions.txt'\n",
    "video_scores_detector2 = {}\n",
    "\n",
    "with open(detector2_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        video_id = parts[0]\n",
    "        score = float(parts[2])\n",
    "        video_scores_detector2[video_id] = score\n",
    "\n",
    "# Initialize combined_video_scores as a defaultdict of lists\n",
    "combined_video_scores = defaultdict(list)\n",
    "\n",
    "# Add scores from the first detector\n",
    "for video_id, score in final_video_scores_detector1.items():\n",
    "    combined_video_scores[video_id].append(score)\n",
    "\n",
    "for video_id, score in video_scores_detector2.items():\n",
    "    combined_video_scores[video_id].append(score)\n",
    "\n",
    "# Calculate the final score for each video by taking the mean of all available scores\n",
    "final_combined_scores = {video_id: np.mean(scores) for video_id, scores in combined_video_scores.items()}\n",
    "\n",
    "# Preparing labels and predictions for AUC calculation\n",
    "true_labels = []\n",
    "pred_scores = []\n",
    "\n",
    "for video_id, score in final_combined_scores.items():\n",
    "    label = video_frame_ground_truth[video_id]\n",
    "    true_labels.append(label)\n",
    "    pred_scores.append(score)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# 讀取txt內容\n",
    "video_predictions = defaultdict(list)\n",
    "\n",
    "labels_dict = {}\n",
    "pattern = r'\\d{2}_\\d{2}'\n",
    "\n",
    "# 读取文件内容\n",
    "file_path = '/home/jovyan/temporaldeepfake/auc_results/spatial/only_train_without45_base_08_05_01_40_56_weights_22_0.7602_val.tar_DFD_fake_predict.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 處理每一行\n",
    "for line in lines:\n",
    "    parts = line.split(maxsplit=2)  # 只分割前两部分，保留预测值的完整性\n",
    "    video_id = parts[0]\n",
    "    probabilities = ast.literal_eval(parts[2])\n",
    "    if isinstance(probabilities, list):\n",
    "        max_prediction = max(probabilities)\n",
    "    else:\n",
    "        max_prediction = probabilities\n",
    "    video_predictions[video_id].append(max_prediction)\n",
    "        # 生成标签：包含两个不同ID的标为1，否则为0\n",
    "    print( re.search(pattern, video_id))\n",
    "    if video_id not in labels_dict:\n",
    "        labels_dict[video_id] = 1 if re.search(pattern, video_id) is not None else 0\n",
    "\n",
    "# 計算每個video從1到32的預測topk的平均並寫入文件\n",
    "for k in [10 , 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130]:\n",
    "    topk_averages = {}\n",
    "    for video_id, predictions in video_predictions.items():\n",
    "        sorted_predictions = sorted(predictions, reverse=True)\n",
    "        topk_averages[video_id] = np.mean(sorted_predictions[:k])\n",
    "    \n",
    "    labels = []\n",
    "    scores = []\n",
    "    with open(f'top{k}_predictions.txt', 'w') as output_file:\n",
    "        for video_id, avg_score in topk_averages.items():\n",
    "            label = labels_dict[video_id]\n",
    "            labels.append(label)\n",
    "            scores.append(avg_score)\n",
    "            output_file.write(f'{video_id} {avg_score} {label}\\n')\n",
    "    \n",
    "    # 計算並打印AUC\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    print(f\"K={k}, AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取第一个检测器的预测结果文件\n",
    "for k in range(1, 26):\n",
    "    detector1_file_path = \"/home/jovyan/temporaldeepfake/auc_results/robustness/temporal/robustness_JPEG_5.txt\"\n",
    "    video_frame_scores = defaultdict(list)\n",
    "    with open(detector1_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                full_path = parts[0]\n",
    "                score = float(parts[1])\n",
    "                ground_truth = int(parts[2])\n",
    "\n",
    "                # 提取影片、frame和face id\n",
    "                path_parts = full_path.split('/')\n",
    "                video_id = path_parts[-3]  # 影片ID，例如000_003\n",
    "                frame_id = path_parts[-2]  # frame ID，例如008\n",
    "\n",
    "                # 使用影片和frame ID作为键来存储预测分数\n",
    "                video_frame_scores[(video_id, frame_id)].append(score)\n",
    "\n",
    "    # 计算每个影片的frame的最大预测分数的平均值\n",
    "    video_scores_detector1 = defaultdict(list)\n",
    "    for (video_id, frame_id), scores in video_frame_scores.items():\n",
    "        max_score = scores[0]\n",
    "        video_scores_detector1[video_id].append(max_score)\n",
    "\n",
    "    # 计算每个影片的最终预测结果\n",
    "    final_video_scores_detector1 = {}\n",
    "    for video_id, scores in video_scores_detector1.items():\n",
    "        top_k_scores = sorted(scores, reverse=True)[:k]\n",
    "        average_top_k = sum(top_k_scores) / len(top_k_scores)\n",
    "        final_video_scores_detector1[video_id] = average_top_k\n",
    "\n",
    "\n",
    "\n",
    "    # # 读取第二个检测器的预测结果文件\n",
    "    detector2_file_path = '/home/jovyan/temporaldeepfake/auc_results/robustness/spatial/JPEG_5_top128_predictions.txt'\n",
    "    video_scores_detector2 = {}\n",
    "\n",
    "    with open(detector2_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            video_id = parts[0]\n",
    "            score = float(parts[1])\n",
    "            video_scores_detector2[video_id] = score\n",
    "\n",
    "    # Initialize combined_video_scores as a defaultdict of lists\n",
    "    combined_video_scores = defaultdict(list)\n",
    "\n",
    "    #Add scores from the first detector\n",
    "    for video_id, score in final_video_scores_detector1.items():\n",
    "        combined_video_scores[video_id].append(score)\n",
    "\n",
    "    # Add scores from the second detector\n",
    "    for video_id, score in video_scores_detector2.items():\n",
    "        combined_video_scores[video_id].append(score)\n",
    "\n",
    "    # Calculate the final score for each video by taking the mean of all available scores\n",
    "    final_combined_scores = {video_id: np.mean(scores) for video_id, scores in combined_video_scores.items()}\n",
    "\n",
    "    # Preparing labels and predictions for AUC calculation\n",
    "    true_labels = []\n",
    "    pred_scores = []\n",
    "    \n",
    "    for video_id, score in final_combined_scores.items():\n",
    "        label = 1 if '_' in video_id else 0  # 假设video_id中含有两个下划线表示id\n",
    "        true_labels.append(label)\n",
    "        pred_scores.append(score)\n",
    "    # Calculate AUC\n",
    "    if len(set(true_labels)) > 1:  # 确保标签中有至少两个不同值\n",
    "        auc = roc_auc_score(true_labels, pred_scores)\n",
    "        print(f\"{auc}\")\n",
    "    else:\n",
    "        print(f\"top{k}: Not enough different labels to calculate AUC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import ast\n",
    "\n",
    "# 讀取txt內容\n",
    "video_predictions = defaultdict(list)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "# 读取文件内容\n",
    "file_path = '/home/jovyan/temporaldeepfake/auc_results/robustness/spatial/robustness_base_08_15_17_48_26_weights_172_0.9855_val.tar_FaceForensics++_JPEG_3_predict.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 處理每一行\n",
    "for line in lines:\n",
    "    parts = line.split(maxsplit=2)  # 只分割前两部分，保留预测值的完整性\n",
    "    video_id = parts[0]\n",
    "    probabilities = ast.literal_eval(parts[2])\n",
    "    if isinstance(probabilities, list):\n",
    "        max_prediction = max(probabilities)\n",
    "    else:\n",
    "        max_prediction = probabilities\n",
    "    video_predictions[video_id].append(max_prediction)\n",
    "        # 生成标签：包含两个不同ID的标为1，否则为0\n",
    "    if video_id not in labels_dict:\n",
    "        labels_dict[video_id] = 1 if '_' in video_id else 0\n",
    "print(len(video_predictions))\n",
    "print(video_predictions)\n",
    "# 計算每個video從1到32的預測topk的平均並寫入文件\n",
    "for k in [32, 64, 128 , 256]:\n",
    "    topk_averages = {}\n",
    "    for video_id, predictions in video_predictions.items():\n",
    "        sorted_predictions = sorted(predictions, reverse=True)\n",
    "        topk_averages[video_id] = np.mean(sorted_predictions[:k])\n",
    "    \n",
    "    labels = []\n",
    "    scores = []\n",
    "    with open(f'/home/jovyan/temporaldeepfake/auc_results/robustness/spatial/JPEG_3_top{k}_predictions.txt', 'w') as output_file:\n",
    "        for video_id, avg_score in topk_averages.items():\n",
    "            label = labels_dict[video_id]\n",
    "            labels.append(label)\n",
    "            scores.append(avg_score)\n",
    "            output_file.write(f'{video_id} {avg_score} {label}\\n')\n",
    "    \n",
    "    # 計算並打印AUC\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    print(f\"K={k}, AUC: {auc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
